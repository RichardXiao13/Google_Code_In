# -*- coding: utf-8 -*-
"""Background_Remover.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SVovHT1VtzxPTsJ7w221St1nZBaZv8W1

Import tensorflow.
"""

# Commented out IPython magic to ensure Python compatibility.
try:
#   %tensorflow_version 2.x
except:
  Exception
import tensorflow as tf

"""Import layers from tensorflow. keras API."""

import tensorflow.keras.layers as layers

"""Define a function that creates the blocks of layers specified for the tiramisu model. It consists of Batch Normalization, ReLU, 3 x 3 Convolution, and a Dropout layer where p = 0.2."""

def block(x, num_layers):
  for i in range(num_layers):
    t = x
    x = layers.BatchNormalization(axis=1, beta_regularizer=tf.keras.regularizers.l2(0.0001), gamma_regularizer=tf.keras.regularizers.l2(0.0001))(x)
    x = layers.Activation("relu")(x)
    x = layers.Conv2D(16, (3, 3), padding="same", kernel_initializer="he_uniform")(x)
    x = layers.Dropout(0.2)(x)
    x = layers.concatenate([x, t])
  return x

"""Define a transition down function which creates the layers in the transition down blocks for the model. It consists of Batch Normalization, ReLU, 1 x 1 Convolution, Dropout where p = 0.2, and a 2 x 2 Max Pooling layer."""

def transition_down(x, num_features):
    x = layers.BatchNormalization(axis=1, beta_regularizer=tf.keras.regularizers.l2(0.0001), gamma_regularizer=tf.keras.regularizers.l2(0.0001))(x)
    x = layers.Activation("relu")(x)
    x = layers.Conv2D(num_features, (1, 1), padding="same", kernel_initializer="he_uniform")(x)
    x = layers.Dropout(0.2)(x)
    x = layers.MaxPooling2D((2, 2), strides=2, padding="same")(x)
    return x

"""Define a transition up function which creates the layers for the transition up blocks in the model. It consists of a 3 x 3 Transposed Convolution where the stride = 2."""

def transition_up(x, num_features):
  x = layers.Conv2DTranspose(num_features, strides=2, kernel_size=(3, 3), padding="same")(x)
  return x

"""Define a function that creates a tiramisu model. The transition up blocks are contained within this function."""

def create_tiramisu(layer_per_block):
  n_pool = 5
  growth_rate = 16
  num_features = 48
  input_layer = layers.Input((224, 224, 3))
  x = layers.Conv2D(48, (3, 3), padding="same")(input_layer)
  skip_connections = []
  for i in range(n_pool):
    x = block(x, layer_per_block[i])
    skip_connections.append(x)
    num_features += growth_rate * layer_per_block[i]
    x = transition_down(x, num_features)

  x = block(x, layer_per_block[n_pool])
  skip_connections = skip_connections[::-1]

  for i in range(n_pool):
    num_features = growth_rate * layer_per_block[n_pool + i]
    x = transition_up(x, num_features)
    x = layers.concatenate([x, skip_connections[i]])
    x = block(x, layer_per_block[n_pool+i+1])

  x = layers.Conv2D(2, kernel_size=(1, 1), padding='same', kernel_initializer="he_uniform")(x)
  output_layer = layers.Activation('softmax')(x)
  return tf.keras.Model(inputs=input_layer, outputs=output_layer)

"""Create the tiramisu model with 103 layers."""

model_layers = [4, 5, 7, 10, 12, 15, 12, 10, 7, 5, 4]
tiramisu = create_tiramisu(model_layers)

"""Download the Matting Human Datasets for segmentation. Then extract the zip into the workspace."""

import os

os.environ['KAGGLE_USERNAME'] = "richardx13"
os.environ['KAGGLE_KEY'] = "a27f04dddeebb3312cf29c0463cd0ab0"
!kaggle datasets download -d laurentmih/aisegmentcom-matting-human-datasets

import zipfile
with zipfile.ZipFile("/content/aisegmentcom-matting-human-datasets.zip","r") as zip_ref:
    zip_ref.extractall("/content/")

"""Create a function that reads in the folder with the PNG images to extract the alpha layers from them."""

import matplotlib.pyplot as plt
import numpy as np

def extract_alpha(directory):
  alphas = []
  for folder1 in sorted(os.listdir(directory)):
    print(folder1)
    for filename in sorted(os.listdir(os.path.join(directory, folder1)))[:30]:
      img = tf.image.resize(plt.imread(os.path.join(directory, folder1, filename)), (224, 224))
      img = img[:,:,3]
      img = img.numpy()
      for i in range(224):
        for j in range(224):
          img[i,j] = round(img[i,j])
      alphas.append(img)
  return alphas

alphas = extract_alpha("/content/matting_human_half/matting/1803151818/")

plt.imshow(alphas[0])

"""Define a function that stores the corresponding images to the alpha maps extracted above in JPG format."""

def extract_imgs(directory):
  imgs = []
  for folder1 in sorted(os.listdir(directory)):
    print(folder1)
    for filename in sorted(os.listdir(os.path.join(directory, folder1)))[:30]:
      img = tf.image.resize(plt.imread(os.path.join(directory, folder1, filename)), (224, 224))/255
      imgs.append(img)
  return imgs

imgs = extract_imgs("/content/matting_human_half/clip_img/1803151818")

plt.imshow(imgs[0])

"""Create a dataset that takes the images as labels and the alpha maps as outputs. Then batch the dataset."""

dataset = tf.data.Dataset.from_tensor_slices((imgs, alphas))

train_set = dataset.batch(10).prefetch(1)

"""Compile the model and set the training for 5 epochs."""

EPOCHS = 5

tiramisu.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

"""Train the model on the segmentations. Once training is finished, save the model as a .h5 file."""

history = tiramisu.fit(train_set, epochs=EPOCHS)

tiramisu.save("new_tiramisu.h5")

"""Now, we choose some test data to see how well our model performs."""

img_path = "/content/matting_human_half/matting/1803151818/matting_00000001"
imgs_test = []
for img in os.listdir(img_path)[:5]:
  image = plt.imread(os.path.join(img_path, img))
  imgs_test.append(tf.image.resize(image, (224, 224)))

jpg_path = "/content/matting_human_half/clip_img/1803151818/clip_00000001"
jpg_test = []
for img in os.listdir(jpg_path)[:5]:
  image = plt.imread(os.path.join(jpg_path, img))
  jpg_test.append(tf.image.resize(image, (224, 224)))

"""Here, we put the test data into a dataset and batch it for predictions."""

test_data = tf.data.Dataset.from_tensor_slices((jpg_test))
test_data = test_data.batch(len(jpg_test))

predictions = tiramisu.predict(test_data)

"""# **True Labels**"""

plt.figure(figsize=(12, 12))

for i in range(len(imgs_test)):
  plt.subplot(1, 5, i+1)
  plt.imshow(imgs_test[i])

"""# **Corresponding Predictions**"""

plt.figure(figsize=(12, 12))

for i in range(len(imgs_test)):
  mask = tf.argmax(predictions[i], axis=-1)
  imgs_test[i] = imgs_test[i].numpy()
  imgs_test[i][:,:,3] = mask
  plt.subplot(1, 5, i+1)
  plt.imshow(imgs_test[i])

plt.show()

"""# **Results**
Looking at the predictions, we can see that our model didn't perform as well as we would have liked. Some solutions to this would be to use more training data. In this test, we chose a small subset of data to reduce training times from Colab's restrictions. We could also increase the batch size for better generalization while also increasing the training epoch count. Then, we could use a validation set to see if our model was overfitting and when it would occur.
"""